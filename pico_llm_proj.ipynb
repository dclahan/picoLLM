{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# pico-LLM project notebook"
      ],
      "metadata": {
        "id": "VG16ScIxVarI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## imports"
      ],
      "metadata": {
        "id": "iwbwJ6bYVfB-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCISZD5TUoKo"
      },
      "outputs": [],
      "source": [
        "# starter code by matus & o1-pro\n",
        "import argparse\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# We do not import numpy or scikit-learn, so we implement a naive k-means in pure PyTorch.\n",
        "# If you prefer scikit-learn, you can adapt the code.\n",
        "\n",
        "from datasets import load_dataset\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Command-line arg parsing"
      ],
      "metadata": {
        "id": "MfNzPFd5Vh_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# 1. Command-line arg parsing\n",
        "################################################################################\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Train multiple k-gram or sequence-based models on TinyStories and/or custom text files.\")\n",
        "    parser.add_argument(\"--input_files\", nargs=\"*\", default=None,\n",
        "                        help=\"Optional list of text files to mix in as data sources. Each line is one example (up to block_size).\")\n",
        "    parser.add_argument(\"--tinystories_weight\", type=float, default=0.5,\n",
        "                        help=\"Probability of sampling from TinyStories if present. Default=0.5. (set to 0.0 to skip TinyStories).\")\n",
        "    parser.add_argument(\"--max_steps_per_epoch\", type=int, default=None,\n",
        "                        help=\"If set, each epoch ends after this many steps (for quick tests).\")\n",
        "    parser.add_argument(\"--num_inner_mlp_layers\", type=int, default=1,\n",
        "                        help=\"Number of (Linear->SiLU) blocks inside the k-gram MLP. Default=1.\")\n",
        "    parser.add_argument(\"--monosemantic_enabled\", action=\"store_true\",\n",
        "                        help=\"(DISABLED BY DEFAULT) If set, run the monosemantic analysis.\")\n",
        "    parser.set_defaults(monosemantic_enabled=False)  # disable by default\n",
        "\n",
        "    # Additional hyperparams to mitigate slow k-gram\n",
        "    parser.add_argument(\"--kgram_k\", type=int, default=3,\n",
        "                        help=\"Sliding window size for k-gram MLP. Smaller can reduce memory usage. Default=3.\")\n",
        "    parser.add_argument(\"--kgram_chunk_size\", type=int, default=1,\n",
        "                        help=\"Process k-gram timesteps in micro-batches. Default=1.\")\n",
        "\n",
        "    parser.add_argument(\"--block_size\", type=int, default=1024,\n",
        "                        help=\"Maximum sequence length for each example. Default=1024.\")\n",
        "\n",
        "    # New arguments:\n",
        "    parser.add_argument(\"--embed_size\", type=int, default=1024,\n",
        "                        help=\"Dimension of the embedding layer for LSTM, MLP, etc. Default=1024.\")\n",
        "    parser.add_argument(\"--prompt\", type=str, default=\"Once upon a\",\n",
        "                        help=\"Prompt used for generation. Default='Once upon a'.\")\n",
        "\n",
        "    # Newly added device argument:\n",
        "    parser.add_argument(\"--device_id\", type=str, default=\"cuda:0\",\n",
        "                        help=\"Torch device identifier (default='cuda:0'). If CUDA is unavailable, fallback to 'cpu'.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args"
      ],
      "metadata": {
        "id": "oFIbsVK_VX5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. data handling"
      ],
      "metadata": {
        "id": "3Z2oJYRhVnig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# 2. Data handling: entire sequences up to block_size => (seq_len, batch)\n",
        "################################################################################\n",
        "\n",
        "class MixedSequenceDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    We store two lists of entire token sequences:\n",
        "      - tinystories_seqs\n",
        "      - other_seqs\n",
        "    Each sequence is length <= block_size.\n",
        "\n",
        "    During __getitem__, we randomly pick from one list or the other with probability p_tiny.\n",
        "    Return that entire sequence as a 1D LongTensor.\n",
        "    \"\"\"\n",
        "    def __init__(self, tinystories_seqs, other_seqs, p_tiny: float):\n",
        "        super().__init__()\n",
        "        self.tinystories_seqs = tinystories_seqs\n",
        "        self.other_seqs = other_seqs\n",
        "        self.p_tiny = p_tiny\n",
        "\n",
        "        self.has_tinystories = (len(self.tinystories_seqs) > 0)\n",
        "        self.has_other = (len(self.other_seqs) > 0)\n",
        "\n",
        "        self.total_length = len(self.tinystories_seqs) + len(self.other_seqs)\n",
        "        if self.total_length == 0:\n",
        "            raise ValueError(\"No data found! Both TinyStories and other sets are empty.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = random.random()\n",
        "        if self.has_tinystories and self.has_other:\n",
        "            if r < self.p_tiny:\n",
        "                i = random.randint(0, len(self.tinystories_seqs) - 1)\n",
        "                seq = self.tinystories_seqs[i]\n",
        "            else:\n",
        "                i = random.randint(0, len(self.other_seqs) - 1)\n",
        "                seq = self.other_seqs[i]\n",
        "        elif self.has_tinystories:\n",
        "            i = random.randint(0, len(self.tinystories_seqs) - 1)\n",
        "            seq = self.tinystories_seqs[i]\n",
        "        else:\n",
        "            i = random.randint(0, len(self.other_seqs) - 1)\n",
        "            seq = self.other_seqs[i]\n",
        "\n",
        "        return torch.tensor(seq, dtype=torch.long)\n",
        "\n",
        "\n",
        "def seq_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    batch: list of 1D LongTensors of various lengths [<= block_size].\n",
        "    1) find max length\n",
        "    2) pad with zeros\n",
        "    3) shape => (max_len, batch_size)\n",
        "    \"\"\"\n",
        "    max_len = max(len(seq) for seq in batch)\n",
        "    batch_size = len(batch)\n",
        "\n",
        "    padded = torch.zeros(max_len, batch_size, dtype=torch.long)\n",
        "    for i, seq in enumerate(batch):\n",
        "        seq_len = seq.size(0)\n",
        "        padded[:seq_len, i] = seq\n",
        "\n",
        "    return padded\n"
      ],
      "metadata": {
        "id": "oRX7EjYLVTi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. K-gram MLP in seq-2-seq approach"
      ],
      "metadata": {
        "id": "6DIBibiYVqdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# 3. K-gram MLP in a sequence-to-sequence approach\n",
        "################################################################################\n",
        "\n",
        "def compute_next_token_loss(logits, tokens):\n",
        "    \"\"\"\n",
        "    logits: (seq_len, batch, vocab_size)\n",
        "    tokens: (seq_len, batch)\n",
        "    Next-token prediction => we shift target by 1.\n",
        "    \"\"\"\n",
        "    seq_len, batch_size, vocab_size = logits.shape\n",
        "    if seq_len < 2:\n",
        "        return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
        "\n",
        "    preds = logits[:-1, :, :]  # (seq_len-1, batch, vocab_size)\n",
        "    gold = tokens[1:, :]       # (seq_len-1, batch)\n",
        "\n",
        "    preds = preds.reshape(-1, vocab_size)\n",
        "    gold = gold.reshape(-1)\n",
        "    return F.cross_entropy(preds, gold)\n",
        "\n",
        "\n",
        "class KGramMLPSeqModel(nn.Module):\n",
        "    \"\"\"\n",
        "    For each position t in [0..seq_len-1], gather the last k tokens => one-hot => MLP => logits.\n",
        "    Return (seq_len, batch, vocab_size).\n",
        "\n",
        "    Potentially very large memory usage for big vocab or seq_len. chunk_size helps mitigate overhead.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, k=3, embed_size=1024, num_inner_layers=1, chunk_size=1):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.num_inner_layers = num_inner_layers\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "        # fill in\n",
        "\n",
        "        self.net = None\n",
        "\n",
        "    def forward(self, tokens_seq):\n",
        "        \"\"\"\n",
        "        tokens_seq: (seq_len, batch)\n",
        "        return: (seq_len, batch, vocab_size)\n",
        "        We'll do a loop over time steps. chunk_size can reduce overhead.\n",
        "        \"\"\"\n",
        "        seq_len, batch_size = tokens_seq.shape\n",
        "        outputs = []\n",
        "\n",
        "        start = 0\n",
        "        while start < seq_len:\n",
        "            end = min(start + self.chunk_size, seq_len)\n",
        "            block_outputs = []\n",
        "            for t in range(start, end):\n",
        "                batch_logits = []\n",
        "                for b in range(batch_size):\n",
        "                    if t < self.k:\n",
        "                        needed = self.k - t\n",
        "                        context_ids = [0]*needed + tokens_seq[:t, b].tolist()\n",
        "                    else:\n",
        "                        context_ids = tokens_seq[t-self.k:t, b].tolist()\n",
        "\n",
        "                    context_oh = F.one_hot(\n",
        "                        torch.tensor(context_ids, dtype=torch.long, device=tokens_seq.device),\n",
        "                        num_classes=self.vocab_size\n",
        "                    )\n",
        "                    context_flat = context_oh.flatten().float().unsqueeze(0)\n",
        "                    logits_b = self.net(context_flat)  # (1, vocab_size)\n",
        "                    batch_logits.append(logits_b)\n",
        "                block_outputs.append(torch.cat(batch_logits, dim=0).unsqueeze(0))  # (1, batch, vocab_size)\n",
        "\n",
        "            block_outputs = torch.cat(block_outputs, dim=0)  # (chunk_size, batch, vocab_size)\n",
        "            outputs.append(block_outputs)\n",
        "            start = end\n",
        "\n",
        "        outputs = torch.cat(outputs, dim=0)  # (seq_len, batch, vocab_size)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "UfvU8yD0VO6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. LSTM-based seq2seq"
      ],
      "metadata": {
        "id": "n2xNZTGIVu4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# 4. LSTM-based seq2seq\n",
        "################################################################################\n",
        "\n",
        "class LSTMSeqModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=1024, hidden_size=1024):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=False)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, tokens_seq):\n",
        "        \"\"\"\n",
        "        tokens_seq: (seq_len, batch)\n",
        "        => (seq_len, batch, vocab_size)\n",
        "        \"\"\"\n",
        "        emb = self.embedding(tokens_seq)   # (seq_len, batch, embed)\n",
        "        self.lstm.flatten_parameters()\n",
        "        out, _ = self.lstm(emb)           # (seq_len, batch, hidden)\n",
        "        logits = self.linear(out)         # (seq_len, batch, vocab_size)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "GBdQ-Cq7VL0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Stub transformer w KV-cache"
      ],
      "metadata": {
        "id": "5Bd9MuEzVyZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# 5. Our \"stub\" Transformer with KV-cache\n",
        "#    Very slow Python loop for training. Multi-head sums head outputs.\n",
        "################################################################################\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x):\n",
        "        norm = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).sqrt()\n",
        "        return self.weight * (x / norm)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size=50257, d_model=1024, n_heads=2, n_blocks=4):\n",
        "        super().__init__()\n",
        "        pass"
      ],
      "metadata": {
        "id": "VjMEb01SVIom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. K-means monosemantic"
      ],
      "metadata": {
        "id": "6MiGvysdV54T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# 6. K-Means Monosemantic (DISABLED by default)\n",
        "################################################################################\n",
        "\n",
        "\n",
        "def monosemantic_analysis_for_token(token_id, model, enc, device=\"cpu\", top_n=5):\n",
        "    return []"
      ],
      "metadata": {
        "id": "E0kjdCFnVCHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. single code path for text-gen"
      ],
      "metadata": {
        "id": "fYmJGkXXV9Gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# 7. Single code path for text generation\n",
        "################################################################################\n",
        "\n",
        "def nucleus_sampling(logits, p=0.95):\n",
        "    return torch.argmax(logits).item()\n",
        "\n",
        "\n",
        "def generate_text(model, enc, init_text, max_new_tokens=20, device=\"cpu\",\n",
        "                  top_p=None,\n",
        "                  monosemantic_info=None,\n",
        "                  do_monosemantic=False):\n",
        "    \"\"\"\n",
        "    A single code path for all models:\n",
        "      - We keep a growing list 'context_tokens'.\n",
        "      - At each step, we feed the entire context as (seq_len,1) to model(...).\n",
        "      - We get model(...)->(seq_len,1,vocab_size). We take the final step's logits => logits[-1,0,:].\n",
        "      - We pick next token (greedy or top-p), append to context_tokens.\n",
        "      - Optionally do monosemantic analysis on that newly generated token.\n",
        "    \"\"\"\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        context_tokens = enc.encode(init_text)\n",
        "        annotation_list = []\n",
        "\n",
        "        for step_i in range(max_new_tokens):\n",
        "            seq_tensor = torch.tensor(context_tokens, dtype=torch.long, device=device).unsqueeze(1)\n",
        "            logits_seq = model(seq_tensor)              # (seq_len,1,vocab_size)\n",
        "            next_logits = logits_seq[-1, 0, :]         # shape (vocab_size,)\n",
        "\n",
        "            if top_p is None:\n",
        "                # greedy\n",
        "                chosen_token = torch.argmax(next_logits).item()\n",
        "            else:\n",
        "                chosen_token = nucleus_sampling(next_logits, p=top_p)\n",
        "\n",
        "            context_tokens.append(chosen_token)\n",
        "\n",
        "            if do_monosemantic and monosemantic_info is not None:\n",
        "                neighbors = monosemantic_analysis_for_token(\n",
        "                    chosen_token, model, monosemantic_info, enc, device=device, top_n=5\n",
        "                )\n",
        "                annotation_list.append((chosen_token, neighbors))\n",
        "            else:\n",
        "                annotation_list.append((chosen_token, []))\n",
        "\n",
        "    model.train(was_training)\n",
        "\n",
        "    final_text = enc.decode(context_tokens)\n",
        "    prefix_text = enc.decode(context_tokens[:-max_new_tokens])\n",
        "    annotated_strs = [prefix_text]\n",
        "    for (tid, neighs) in annotation_list:\n",
        "        token_str = enc.decode([tid])\n",
        "        if neighs:\n",
        "            neighbor_strs = [f\"{enc.decode([x[1]])}\" for x in neighs]\n",
        "            annotated = f\"{token_str}[NN={neighbor_strs}]\"\n",
        "        else:\n",
        "            annotated = token_str\n",
        "        annotated_strs.append(annotated)\n",
        "\n",
        "    annotated_text = \"\".join(annotated_strs)\n",
        "    return final_text, annotated_text"
      ],
      "metadata": {
        "id": "zcKtp61iU_WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. training"
      ],
      "metadata": {
        "id": "t2DwsfCUWA4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# 8. Training\n",
        "################################################################################\n",
        "\n",
        "def train_one_model(model,\n",
        "                    loader,\n",
        "                    epochs,\n",
        "                    model_name,\n",
        "                    device,\n",
        "                    lr=1e-3,\n",
        "                    log_steps=100,\n",
        "                    sample_interval=30,\n",
        "                    max_steps_per_epoch=None,\n",
        "                    enc=None,\n",
        "                    monosemantic_info=None,\n",
        "                    prompt=\"Once upon a\"):\n",
        "    \"\"\"\n",
        "    We add `prompt` as an explicit argument so we can pass it down from main().\n",
        "    \"\"\"\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    start_time = time.time()\n",
        "    next_sample_time = start_time\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        partial_loss = 0.0\n",
        "        partial_count = 0\n",
        "\n",
        "        step_in_epoch = 0\n",
        "        for batch_idx, batch_tokens in enumerate(loader, start=1):\n",
        "            step_in_epoch += 1\n",
        "            global_step += 1\n",
        "\n",
        "            batch_tokens = batch_tokens.to(device)  # (seq_len, batch)\n",
        "\n",
        "            logits = model(batch_tokens)  # (seq_len, batch, vocab_size)\n",
        "            loss = compute_next_token_loss(logits, batch_tokens)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            partial_loss += loss.item()\n",
        "            partial_count += 1\n",
        "\n",
        "            if batch_idx % log_steps == 0:\n",
        "                avg_part_loss = partial_loss / partial_count\n",
        "                print(f\"[{model_name}] Epoch {epoch}/{epochs}, \"\n",
        "                      f\"Step {batch_idx}/{len(loader)} (global step: {global_step}) \"\n",
        "                      f\"Partial Avg Loss: {avg_part_loss:.4f}\")\n",
        "                partial_loss = 0.0\n",
        "                partial_count = 0\n",
        "\n",
        "            current_time = time.time()\n",
        "            if current_time >= next_sample_time and enc is not None:\n",
        "                with torch.no_grad():\n",
        "                    print(f\"\\n[{model_name}] Generating sample text (greedy) at epoch={epoch}, step={batch_idx}...\")\n",
        "                    text_greedy, ann_greedy = generate_text(\n",
        "                        model, enc, prompt, max_new_tokens=20, device=device,\n",
        "                        top_p=None,\n",
        "                        monosemantic_info=monosemantic_info,\n",
        "                        do_monosemantic=(monosemantic_info is not None)\n",
        "                    )\n",
        "                    print(f\" Greedy Sample: {text_greedy}\")\n",
        "                    print(f\" Annotated: {ann_greedy}\\n\")\n",
        "\n",
        "                    print(f\"[{model_name}] Generating sample text (top-p=0.95) at epoch={epoch}, step={batch_idx}...\")\n",
        "                    text_topp, ann_topp = generate_text(\n",
        "                        model, enc, prompt, max_new_tokens=20, device=device,\n",
        "                        top_p=0.95,\n",
        "                        monosemantic_info=monosemantic_info,\n",
        "                        do_monosemantic=(monosemantic_info is not None)\n",
        "                    )\n",
        "                    print(f\" Top-p (p=0.95) Sample: {text_topp}\")\n",
        "                    print(f\" Annotated: {ann_topp}\\n\")\n",
        "\n",
        "                    # third generation => top-p=1.0 => full distribution random sampling\n",
        "                    print(f\"[{model_name}] Generating sample text (top-p=1.0) at epoch={epoch}, step={batch_idx}...\")\n",
        "                    text_topp1, ann_topp1 = generate_text(\n",
        "                        model, enc, prompt, max_new_tokens=20, device=device,\n",
        "                        top_p=1.0,\n",
        "                        monosemantic_info=monosemantic_info,\n",
        "                        do_monosemantic=(monosemantic_info is not None)\n",
        "                    )\n",
        "                    print(f\" Top-p (p=1.0) Sample: {text_topp1}\")\n",
        "                    print(f\" Annotated: {ann_topp1}\\n\")\n",
        "\n",
        "                next_sample_time = current_time + sample_interval\n",
        "\n",
        "            if max_steps_per_epoch is not None and step_in_epoch >= max_steps_per_epoch:\n",
        "                print(f\"[{model_name}] Reached max_steps_per_epoch={max_steps_per_epoch}, ending epoch {epoch} early.\")\n",
        "                break\n",
        "\n",
        "        avg_loss = total_loss / step_in_epoch\n",
        "        print(f\"[{model_name}] *** End of Epoch {epoch} *** Avg Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "epIPkkXXU7b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9. main"
      ],
      "metadata": {
        "id": "fgTdtTNjWCnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# 9. Main\n",
        "################################################################################\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    # Additional local variables from arguments\n",
        "    k = args.kgram_k\n",
        "    chunk_size = args.kgram_chunk_size\n",
        "\n",
        "    embed_size = args.embed_size\n",
        "    batch_size = 16\n",
        "    num_epochs = 3\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    block_size = args.block_size\n",
        "    train_subset_size = 20000\n",
        "    log_interval_steps = 100\n",
        "    sample_interval_seconds = 30\n",
        "\n",
        "    max_steps_per_epoch = args.max_steps_per_epoch\n",
        "    num_inner_layers = args.num_inner_mlp_layers\n",
        "\n",
        "    # NEW: pick device from args.device_id, fallback to cpu if needed\n",
        "    requested_device_id = args.device_id\n",
        "    if requested_device_id.startswith(\"cuda\") and not torch.cuda.is_available():\n",
        "        print(f\"Requested device '{requested_device_id}' but CUDA not available. Falling back to CPU.\")\n",
        "        device = torch.device(\"cpu\")\n",
        "    else:\n",
        "        device = torch.device(requested_device_id)\n",
        "\n",
        "    print(f\"Using device: {device}, block_size={block_size}, kgram_k={k}, chunk_size={chunk_size}, embed_size={embed_size}\")\n",
        "\n",
        "    ############################################################################\n",
        "    # Data\n",
        "    ############################################################################\n",
        "    tinystories_seqs = []\n",
        "    other_seqs = []\n",
        "\n",
        "    if args.tinystories_weight > 0.0:\n",
        "        print(f\"Loading TinyStories from huggingface with weight={args.tinystories_weight}...\")\n",
        "        dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
        "        dataset = dataset.select(range(train_subset_size))\n",
        "    else:\n",
        "        print(\"TinyStories weight=0 => skipping TinyStories.\")\n",
        "        dataset = None\n",
        "\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    vocab_size = enc.n_vocab\n",
        "    print(f\"Vocab size: {vocab_size}\")\n",
        "\n",
        "    if dataset is not None:\n",
        "        for sample in dataset:\n",
        "            text = sample['text']\n",
        "            tokens = enc.encode(text)\n",
        "            tokens = tokens[:block_size]\n",
        "            if len(tokens) > 0:\n",
        "                tinystories_seqs.append(tokens)\n",
        "        print(f\"TinyStories sequences: {len(tinystories_seqs)}\")\n",
        "\n",
        "    if args.input_files:\n",
        "        for filepath in args.input_files:\n",
        "            print(f\"Reading custom text file: {filepath}\")\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                lines = f.readlines()\n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                tokens = enc.encode(line)\n",
        "                tokens = tokens[:block_size]\n",
        "                if len(tokens) > 0:\n",
        "                    other_seqs.append(tokens)\n",
        "        print(f\"Custom input files: {len(other_seqs)} sequences loaded.\")\n",
        "    else:\n",
        "        print(\"No custom input files provided.\")\n",
        "\n",
        "    p_tiny = args.tinystories_weight\n",
        "    if len(tinystories_seqs) == 0 and p_tiny>0:\n",
        "        print(\"Warning: TinyStories is empty but tinystories_weight>0. That's okay, no data from it.\")\n",
        "    combined_dataset = MixedSequenceDataset(\n",
        "        tinystories_seqs=tinystories_seqs,\n",
        "        other_seqs=other_seqs,\n",
        "        p_tiny=p_tiny\n",
        "    )\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        combined_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        collate_fn=seq_collate_fn\n",
        "    )\n",
        "\n",
        "    ############################################################################\n",
        "    # Models\n",
        "    ############################################################################\n",
        "    kgram_model = KGramMLPSeqModel(\n",
        "        vocab_size=vocab_size,\n",
        "        k=k,\n",
        "        embed_size=embed_size,\n",
        "        num_inner_layers=num_inner_layers,\n",
        "        chunk_size=chunk_size\n",
        "    ).to(device)\n",
        "\n",
        "    lstm_model = LSTMSeqModel(\n",
        "        vocab_size=vocab_size,\n",
        "        embed_size=embed_size,\n",
        "        hidden_size=embed_size\n",
        "    ).to(device)\n",
        "\n",
        "    transformer = TransformerModel(\n",
        "    ).to(device)\n",
        "\n",
        "    models = {\n",
        "      # \"kgram_mlp_seq\": kgram_model,\n",
        "        \"lstm_seq\": lstm_model,\n",
        "      # \"kvcache_transformer\": kv_transformer,\n",
        "    }\n",
        "\n",
        "\n",
        "    ############################################################################\n",
        "    # Train each model\n",
        "    ############################################################################\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\n=== Training model: {model_name} ===\")\n",
        "        train_one_model(\n",
        "            model=model,\n",
        "            loader=train_loader,\n",
        "            epochs=num_epochs,\n",
        "            model_name=model_name,\n",
        "            device=device,\n",
        "            lr=learning_rate,\n",
        "            log_steps=log_interval_steps,\n",
        "            sample_interval=sample_interval_seconds,\n",
        "            max_steps_per_epoch=max_steps_per_epoch,\n",
        "            enc=enc,\n",
        "            prompt=args.prompt  # <--- Pass the user-specified prompt here\n",
        "        )\n",
        "\n",
        "        # Final generation from the user-provided prompt (args.prompt).\n",
        "        with torch.no_grad():\n",
        "            # 1) Greedy\n",
        "            text_greedy, ann_greedy = generate_text(\n",
        "                model, enc, args.prompt, max_new_tokens=20, device=device,\n",
        "                top_p=None,\n",
        "            )\n",
        "            # 2) top-p=0.95\n",
        "            text_topp, ann_topp = generate_text(\n",
        "                model, enc, args.prompt, max_new_tokens=20, device=device,\n",
        "                top_p=0.95,\n",
        "            )\n",
        "            # 3) top-p=1.0 => full distribution random sampling\n",
        "            text_topp1, ann_topp1 = generate_text(\n",
        "                model, enc, args.prompt, max_new_tokens=20, device=device,\n",
        "                top_p=1.0,\n",
        "            )\n",
        "\n",
        "        print(f\"[{model_name}] Final sample (greedy) from prompt: '{args.prompt}'\")\n",
        "        print(text_greedy)\n",
        "        print(f\"Annotated:\\n{ann_greedy}\\n\")\n",
        "\n",
        "        print(f\"[{model_name}] Final sample (top-p=0.95) from prompt: '{args.prompt}'\")\n",
        "        print(text_topp)\n",
        "        print(f\"Annotated:\\n{ann_topp}\\n\")\n",
        "\n",
        "        print(f\"[{model_name}] Final sample (top-p=1.0) from prompt: '{args.prompt}'\")\n",
        "        print(text_topp1)\n",
        "        print(f\"Annotated:\\n{ann_topp1}\")\n",
        "        print(\"--------------------------------------------------\")\n",
        "\n",
        "    # Finally, let's share how I'm feeling:\n",
        "    print(\"\\n*** I'm feeling great today! Hope you're well, too. ***\")"
      ],
      "metadata": {
        "id": "Ywm3kvmYU1cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "fKElN0fMUquE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}